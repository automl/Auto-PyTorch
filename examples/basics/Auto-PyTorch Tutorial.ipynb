{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial introduces the basic Auto-PyTorch API together with the classes for featurized and image data.\n",
    "So far, Auto-PyTorch covers classification and regression on featurized data as well as classification on image data.\n",
    "For installing Auto-PyTorch, please refer to the github page.\n",
    "\n",
    "**Disclaimer**: In this notebook, data will be downloaded from the openml project for featurized tasks and CIFAR10 will be downloaded for image classification. Hence, an internet connection is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API\n",
    "\n",
    "There are classes for featurized tasks (classification, multi-label classification, regression) and image tasks (classification). You can import them via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoPyTorch import (AutoNetClassification,\n",
    "                         AutoNetMultilabel,\n",
    "                         AutoNetRegression,\n",
    "                         AutoNetImageClassification,\n",
    "                         AutoNetImageClassificationMultipleDatasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other imports for later usage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os as os\n",
    "import openml\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon initialization of a class, you can specify its configuration. Later, you can override its configuration in each fit call. The *config_preset* allows to constrain the search space to one of *tiny_cs, medium_cs* or *full_cs*. These presets can be seen in *core/presets/*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "autonet = AutoNetClassification(config_preset=\"tiny_cs\", result_logger_dir=\"logs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some useful methods provided by the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the current configuration as dict\n",
    "current_configuration = autonet.get_current_autonet_config()\n",
    "\n",
    "# Get the ConfigSpace object with all hyperparameters, conditions, default values and default ranges\n",
    "hyperparameter_search_space = autonet.get_hyperparameter_search_space()\n",
    "\n",
    "# Print all possible configuration options \n",
    "#autonet.print_help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "The most important methods for using Auto-PyTorch are ***fit***, ***refit***, ***score*** and ***predict***.\n",
    "\n",
    "First, we get some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get data from the openml task \"Supervised Classification on credit-g (https://www.openml.org/t/31)\"\n",
    "task = openml.tasks.get_task(task_id=31)\n",
    "X, y = task.get_X_and_y()\n",
    "ind_train, ind_test = task.get_train_test_split_indices()\n",
    "X_train, Y_train = X[ind_train], y[ind_train]\n",
    "X_test, Y_test = X[ind_test], y[ind_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***fit*** is used to search for a good configuration by fitting configurations chosen by the algorithm (by default BOHB). The incumbent configuration is then returned and stored in the class.\n",
    "\n",
    "We recommend to have a look at the possible configuration options first. Some of the most important options allow you to set the budget type (epochs or time), run id and task id for cluster usage, tensorboard logging, seed and more.\n",
    "\n",
    "Here we search for a configuration for 300 seconds with 60-100 s time for fitting each individual configuration.\n",
    "Use the *validation_split* parameter to specify a split size. You can also pass your own validation set\n",
    "via *X_val* and *Y_val*. Use *log_level=\"info\"* or *log_level=\"debug\"* for more detailed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "autonet = AutoNetClassification(config_preset=\"tiny_cs\", result_logger_dir=\"logs/\")\n",
    "# Fit (note that the settings are for demonstration, you might need larger budgets)\n",
    "results_fit = autonet.fit(X_train=X_train,\n",
    "                          Y_train=Y_train,\n",
    "                          validation_split=0.3,\n",
    "                          max_runtime=300,\n",
    "                          min_budget=60,\n",
    "                          max_budget=100,\n",
    "                          refit=True)\n",
    "\n",
    "# Save fit results as json\n",
    "with open(\"logs/results_fit.json\", \"w\") as file:\n",
    "    json.dump(results_fit, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***refit*** allows you to fit a configuration of your choice for a defined time. By default, the incumbent configuration is refitted during a *fit* call using the *max_budget*. However, *refit* might be useful if you want to fit on the full dataset or even another dataset or if you just want to fit a model without searching.\n",
    "\n",
    "You can specify a hyperparameter configuration to fit (if you do not specify a configuration the incumbent configuration from the last fit call will be used):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [13:07:21:autonet] Start autonet with config:\n",
      "{'embeddings': ['none'], 'lr_scheduler': ['cosine_annealing', 'plateau'], 'networks': ['shapedresnet'], 'over_sampling_methods': ['smote'], 'preprocessors': ['none', 'truncated_svd', 'power_transformer'], 'target_size_strategies': ['none', 'upsample', 'median'], 'result_logger_dir': 'logs/', 'budget_type': 'epochs', 'log_level': 'info', 'use_tensorboard_logger': True, 'validation_split': 0.0, 'hyperparameter_search_space_updates': None, 'categorical_features': None, 'dataset_name': None, 'run_id': '0', 'task_id': -1, 'algorithm': 'bohb', 'eta': 3, 'min_workers': 1, 'working_dir': '.', 'network_interface_name': 'eth0', 'memory_limit_mb': 1000000, 'run_worker_on_master_node': True, 'use_pynisher': True, 'refit_validation_split': 0.0, 'cross_validator': 'none', 'cross_validator_args': {}, 'min_budget_for_cv': 0, 'shuffle': True, 'imputation_strategies': ['mean', 'median', 'most_frequent'], 'normalization_strategies': ['none', 'minmax', 'standardize', 'maxabs'], 'under_sampling_methods': ['none', 'random'], 'final_activation': 'softmax', 'initialization_methods': ['default', 'sparse'], 'initializer': 'simple_initializer', 'optimizer': ['adam', 'adamw', 'sgd', 'rmsprop'], 'additional_logs': [], 'optimize_metric': 'accuracy', 'additional_metrics': [], 'loss_modules': ['cross_entropy', 'cross_entropy_weighted'], 'batch_loss_computation_techniques': ['standard', 'mixup'], 'cuda': True, 'torch_num_threads': 1, 'full_eval_each_epoch': False, 'best_over_epochs': False, 'early_stopping_patience': inf, 'early_stopping_reset_parameters': False, 'random_seed': 1103059814, 'min_budget': 5, 'max_budget': 150, 'max_runtime': inf, 'num_iterations': 4, 'cv_splits': 1, 'increase_number_of_trained_datasets': False}\n",
      "[INFO] [13:07:21:autonet] Start Refitting\n",
      "[INFO] [13:07:21:autonet] [AutoNet] No validation set given and either no cross validator given or budget too low for CV. Continue by splitting 0 of training data.\n",
      "[INFO] [13:07:21:autonet] [AutoNet] CV split 0 of 1\n",
      "[INFO] [13:07:25:autonet] Finished train with budget 50.0: Preprocessing took 0s, Training took 4s, Wrap up took 0s. Total time consumption in s: 4\n",
      "[INFO] [13:07:25:autonet] [AutoNet] Done with current split!\n",
      "[INFO] [13:07:25:autonet] Aggregate the results across the splits\n",
      "[INFO] [13:07:25:autonet] Process 1 additional result(s)\n",
      "[INFO] [13:07:25:autonet] Done Refitting\n"
     ]
    }
   ],
   "source": [
    "# Create an autonet\n",
    "autonet_config = {\n",
    "    \"result_logger_dir\" : \"logs/\",\n",
    "    \"budget_type\" : \"epochs\",\n",
    "    \"log_level\" : \"info\", \n",
    "    \"use_tensorboard_logger\" : True,\n",
    "    \"validation_split\" : 0.0\n",
    "    }\n",
    "autonet = AutoNetClassification(**autonet_config)\n",
    "\n",
    "# Sample a random hyperparameter configuration as an example\n",
    "hyperparameter_config = autonet.get_hyperparameter_search_space().sample_configuration().get_dictionary()\n",
    "\n",
    "# Refit with sampled hyperparameter config for 120 s. This time on the full dataset.\n",
    "results_refit = autonet.refit(X_train=X_train,\n",
    "                              Y_train=Y_train,\n",
    "                              X_valid=None,\n",
    "                              Y_valid=None,\n",
    "                              hyperparameter_config=hyperparameter_config,\n",
    "                              autonet_config=autonet.get_current_autonet_config(),\n",
    "                              budget=50)\n",
    "\n",
    "# Save json\n",
    "with open(\"logs/results_refit.json\", \"w\") as file:\n",
    "    json.dump(results_refit, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***pred*** returns the predictions of the incumbent model. ***score*** can be used to evaluate the model on a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "Accuracy score 74.0\n"
     ]
    }
   ],
   "source": [
    "# See how the random configuration performs (often it just predicts 0)\n",
    "score = autonet.score(X_test=X_test, Y_test=Y_test)\n",
    "pred = autonet.predict(X=X_test)\n",
    "\n",
    "print(\"Model prediction:\", pred[0:10])\n",
    "print(\"Accuracy score\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can also get the incumbent model as PyTorch Sequential model via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=20, out_features=34, bias=True)\n",
      "  (1): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (shortcut): Linear(in_features=34, out_features=48, bias=True)\n",
      "      (start_norm): Sequential(\n",
      "        (0): BatchNorm1d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=34, out_features=48, bias=True)\n",
      "        (1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.4477955154159557, inplace=False)\n",
      "        (4): Linear(in_features=48, out_features=48, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=48, out_features=48, bias=True)\n",
      "        (3): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Dropout(p=0.4477955154159557, inplace=False)\n",
      "        (6): Linear(in_features=48, out_features=48, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (shortcut): Linear(in_features=48, out_features=62, bias=True)\n",
      "      (start_norm): Sequential(\n",
      "        (0): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=48, out_features=62, bias=True)\n",
      "        (1): BatchNorm1d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.22389775770797785, inplace=False)\n",
      "        (4): Linear(in_features=62, out_features=62, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): BatchNorm1d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=62, out_features=62, bias=True)\n",
      "        (3): BatchNorm1d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Dropout(p=0.22389775770797785, inplace=False)\n",
      "        (6): Linear(in_features=62, out_features=62, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (shortcut): Linear(in_features=62, out_features=79, bias=True)\n",
      "      (start_norm): Sequential(\n",
      "        (0): BatchNorm1d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=62, out_features=79, bias=True)\n",
      "        (1): BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.0, inplace=False)\n",
      "        (4): Linear(in_features=79, out_features=79, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=79, out_features=79, bias=True)\n",
      "        (3): BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Dropout(p=0.0, inplace=False)\n",
      "        (6): Linear(in_features=79, out_features=79, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (4): BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=79, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pytorch_model = autonet.get_pytorch_model()\n",
    "print(pytorch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurized Data\n",
    "\n",
    "All classes for featurized data (*AutoNetClassification*, *AutoNetMultilabel*, *AutoNetRegression*) can be used as in the example above. The only difference is the type of labels they accept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Data\n",
    "\n",
    "Auto-PyTorch provides two classes for image data. *autonet_image_classification* can be used for classification for images. The *autonet_multi_image_classification* class allows to search for configurations for image classification across multiple datasets. This means Auto-PyTorch will try to choose a configuration that works well on all given datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find BOHB_Multi_KDE, replacing with object\n"
     ]
    }
   ],
   "source": [
    "# Load classes\n",
    "autonet_image_classification = AutoNetImageClassification(config_preset=\"full_cs\", result_logger_dir=\"logs/\")\n",
    "autonet_multi_image_classification = AutoNetImageClassificationMultipleDatasets(config_preset=\"tiny_cs\", result_logger_dir=\"logs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For passing your image data, you have two options (note that arrays are expected):\n",
    "\n",
    "I) Via a path to a comma-separated value file, which in turn contains the paths to the images and the image labels (note header is assumed to be None):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = os.path.abspath(\"../../datasets/example.csv\")\n",
    "\n",
    "X_train = np.array([csv_dir])\n",
    "Y_train = np.array([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II) directly passing the paths to the images and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_dir, header=None)\n",
    "X_train = df.values[:,0]\n",
    "Y_train = df.values[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Make sure you specify *image_root_folders* if the paths to the images are not specified from your current working directory. You can also specify *images_shape* to up- or downscale images.\n",
    "\n",
    "Using the flag *save_checkpoints=True* will save checkpoints to the result directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': -84.48275862068965,\n",
       " 'optimized_hyperparameter_config': {'NetworkSelectorDatasetInfo:darts:auxiliary': False,\n",
       "  'NetworkSelectorDatasetInfo:darts:drop_path_prob': 0.1,\n",
       "  'NetworkSelectorDatasetInfo:darts:init_channels': 36,\n",
       "  'NetworkSelectorDatasetInfo:darts:layers': 20,\n",
       "  'CreateImageDataLoader:batch_size': 58,\n",
       "  'ImageAugmentation:augment': True,\n",
       "  'ImageAugmentation:cutout': True,\n",
       "  'LossModuleSelectorIndices:loss_module': 'cross_entropy',\n",
       "  'NetworkSelectorDatasetInfo:network': 'densenet',\n",
       "  'OptimizerSelector:optimizer': 'adam',\n",
       "  'SimpleLearningrateSchedulerSelector:lr_scheduler': 'adapt',\n",
       "  'SimpleTrainNode:batch_loss_computation_technique': 'standard',\n",
       "  'ImageAugmentation:autoaugment': True,\n",
       "  'ImageAugmentation:cutout_holes': 2,\n",
       "  'ImageAugmentation:fastautoaugment': True,\n",
       "  'ImageAugmentation:length': 17,\n",
       "  'NetworkSelectorDatasetInfo:densenet:blocks': 4,\n",
       "  'NetworkSelectorDatasetInfo:densenet:growth_rate': 28,\n",
       "  'NetworkSelectorDatasetInfo:densenet:layer_in_block_1': 8,\n",
       "  'NetworkSelectorDatasetInfo:densenet:layer_in_block_2': 16,\n",
       "  'NetworkSelectorDatasetInfo:densenet:layer_in_block_3': 49,\n",
       "  'NetworkSelectorDatasetInfo:densenet:use_dropout': False,\n",
       "  'OptimizerSelector:adam:learning_rate': 0.00012377327234853046,\n",
       "  'OptimizerSelector:adam:weight_decay': 0.06147134718475827,\n",
       "  'SimpleLearningrateSchedulerSelector:adapt:T_max': 450,\n",
       "  'SimpleLearningrateSchedulerSelector:adapt:T_mult': 1.189428111774201,\n",
       "  'SimpleLearningrateSchedulerSelector:adapt:patience': 4,\n",
       "  'SimpleLearningrateSchedulerSelector:adapt:threshold': 0.02209366315824298,\n",
       "  'NetworkSelectorDatasetInfo:densenet:layer_in_block_4': 63},\n",
       " 'budget': 400.0,\n",
       " 'info': {}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autonet_image_classification.fit(X_train=X_train,\n",
    "                                 Y_train=Y_train,\n",
    "                                 images_shape=[3,32,32],\n",
    "                                 min_budget=200,\n",
    "                                 max_budget=400,\n",
    "                                 max_runtime=600,\n",
    "                                 save_checkpoints=True,\n",
    "                                 images_root_folders=[os.path.abspath(\"../../datasets/example_images\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto-PyTorch also supports some common datasets. By passing a comma-separated value file with just one line, e.g. \"CIFAR10, 0\" and specifying *default_dataset_download_dir* it will automatically download the data and use it for searching. Supported datasets are CIFAR10, CIFAR100, SVHN and MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 50000\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/cifar-10-python.tar.gz to ./datasets\n",
      "Files already downloaded and verified\n",
      "0 50000\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': -31.03144184934964,\n",
       " 'optimized_hyperparameter_config': {'NetworkSelectorDatasetInfo:darts:auxiliary': False,\n",
       "  'NetworkSelectorDatasetInfo:darts:drop_path_prob': 0.1,\n",
       "  'NetworkSelectorDatasetInfo:darts:init_channels': 36,\n",
       "  'NetworkSelectorDatasetInfo:darts:layers': 20,\n",
       "  'CreateImageDataLoader:batch_size': 55,\n",
       "  'ImageAugmentation:augment': True,\n",
       "  'ImageAugmentation:cutout': False,\n",
       "  'LossModuleSelectorIndices:loss_module': 'cross_entropy',\n",
       "  'NetworkSelectorDatasetInfo:network': 'darts',\n",
       "  'OptimizerSelector:optimizer': 'sgd',\n",
       "  'SimpleLearningrateSchedulerSelector:lr_scheduler': 'adapt',\n",
       "  'SimpleTrainNode:batch_loss_computation_technique': 'mixup',\n",
       "  'ImageAugmentation:autoaugment': False,\n",
       "  'ImageAugmentation:fastautoaugment': True,\n",
       "  'NetworkSelectorDatasetInfo:darts:edge_normal_0': 'avg_pool_3x3',\n",
       "  'NetworkSelectorDatasetInfo:darts:edge_normal_1': 'dil_conv_5x5',\n",
       "  'NetworkSelectorDatasetInfo:darts:edge_reduce_0': 'avg_pool_3x3',\n",
       "  'NetworkSelectorDatasetInfo:darts:edge_reduce_1': 'dil_conv_5x5',\n",
       "  'NetworkSelectorDatasetInfo:darts:inputs_node_normal_3': '1_2',\n",
       "  'NetworkSelectorDatasetInfo:darts:inputs_node_normal_4': '0_2',\n",
       "  'NetworkSelectorDatasetInfo:darts:inputs_node_normal_5': '3_4',\n",
       "  'NetworkSelectorDatasetInfo:darts:inputs_node_reduce_3': '0_1',\n",
       "  'NetworkSelectorDatasetInfo:darts:inputs_node_reduce_4': '0_1',\n",
       "  'NetworkSelectorDatasetInfo:darts:inputs_node_reduce_5': '0_1',\n",
       "  'OptimizerSelector:sgd:learning_rate': 0.056126455704317305,\n",
       "  'OptimizerSelector:sgd:momentum': 0.2554615131836397,\n",
       "  'OptimizerSelector:sgd:weight_decay': 0.0064933176160527645,\n",
       "  'SimpleLearningrateSchedulerSelector:adapt:T_max': 815,\n",
       "  'SimpleLearningrateSchedulerSelector:adapt:T_mult': 1.2809915617006586,\n",
       "  'SimpleLearningrateSchedulerSelector:adapt:patience': 3,\n",
       "  'SimpleLearningrateSchedulerSelector:adapt:threshold': 0.36553138862173745,\n",
       "  'SimpleTrainNode:mixup:alpha': 0.9499156113310157,\n",
       "  'NetworkSelectorDatasetInfo:darts:edge_normal_12': 'sep_conv_3x3',\n",
       "  'NetworkSelectorDatasetInfo:darts:edge_normal_13': 'sep_conv_3x3',\n",
       "  'NetworkSelectorDatasetInfo:darts:edge_normal_3': 'sep_conv_5x5',\n",
       "  'NetworkSelectorDatasetInfo:darts:edge_normal_4': 'skip_connect',\n",
       "  'NetworkSelectorDatasetInfo:darts:edge_normal_5': 'max_pool_3x3',\n",
       "  'NetworkSelectorDatasetInfo:darts:edge_normal_7': 'avg_pool_3x3',\n",
       "  'NetworkSelectorDatasetInfo:darts:edge_reduce_10': 'sep_conv_5x5',\n",
       "  'NetworkSelectorDatasetInfo:darts:edge_reduce_2': 'sep_conv_5x5',\n",
       "  'NetworkSelectorDatasetInfo:darts:edge_reduce_3': 'max_pool_3x3',\n",
       "  'NetworkSelectorDatasetInfo:darts:edge_reduce_5': 'avg_pool_3x3',\n",
       "  'NetworkSelectorDatasetInfo:darts:edge_reduce_6': 'sep_conv_3x3',\n",
       "  'NetworkSelectorDatasetInfo:darts:edge_reduce_9': 'skip_connect'},\n",
       " 'budget': 900.0,\n",
       " 'info': {}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_cifar_csv = os.path.abspath(\"../../datasets/CIFAR10.csv\")\n",
    "\n",
    "autonet_image_classification.fit(X_train=np.array([path_to_cifar_csv]),\n",
    "                                 Y_train=np.array([0]),\n",
    "                                 min_budget=600,\n",
    "                                 max_budget=900,\n",
    "                                 max_runtime=1800,\n",
    "                                 default_dataset_download_dir=\"./datasets\",\n",
    "                                 images_root_folders=[\"./datasets\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For searching across multiple datasets, pass multiple csv files to the corresponding Auto-PyTorch class. Make sure your specify *images_root_folders* for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autonet_multi_image_classification.fit(X_train=np.array([path_to_cifar_csv, csv_dir]),\n",
    "                                       Y_train=np.array([0]),\n",
    "                                       min_budget=1500,\n",
    "                                       max_budget=2000,\n",
    "                                       max_runtime=4000,\n",
    "                                       default_dataset_download_dir=\"./datasets\",\n",
    "                                       images_root_folders=[\"./datasets\", \"./datasets/example_images\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
