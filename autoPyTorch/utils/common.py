import hashlib
from typing import Any, Dict, Iterable, List, NamedTuple, Optional, Sequence, Type, Union

from ConfigSpace.configuration_space import ConfigurationSpace
from ConfigSpace.hyperparameters import (
    CategoricalHyperparameter,
    Constant,
    Hyperparameter,
    UniformFloatHyperparameter,
    UniformIntegerHyperparameter,
)

import numpy as np

import pandas as pd

import scipy.sparse

import torch
from torch.utils.data.dataloader import default_collate

HyperparameterValueType = Union[int, str, float]


class FitRequirement(NamedTuple):
    """
    A class that holds inputs required to fit a pipeline. Also indicates whether
    requirements have to be user specified or are generated by the pipeline itself.

    Attributes:
        name (str): The name of the variable expected in the input dictionary
        supported_types (Iterable[Type]): An iterable of all types that are supported
        user_defined (bool): If false, this requirement does not have to be given to the pipeline
        dataset_property (bool): If True, this requirement is automatically inferred
            by the Dataset class
    """

    name: str
    supported_types: Iterable[Type]
    user_defined: bool
    dataset_property: bool

    def __str__(self) -> str:
        """
        String representation for the requirements
        """
        return "Name: %s | Supported types: %s | User defined: %s | Dataset property: %s" % (
            self.name, self.supported_types, self.user_defined, self.dataset_property)


class HyperparameterSearchSpace(NamedTuple):
    """
    A class that holds the search space for an individual hyperparameter.
    Attributes:
        hyperparameter (str):
            name of the hyperparameter
        value_range (Sequence[HyperparameterValueType]):
            range of the hyperparameter, can be defined as min and
            max values for Numerical hyperparameter or a list of
            choices for a Categorical hyperparameter
        default_value (HyperparameterValueType):
            default value of the hyperparameter
        log (bool):
            whether to sample hyperparameter on a log scale
    """
    hyperparameter: str
    value_range: Sequence[HyperparameterValueType]
    default_value: HyperparameterValueType
    log: bool = False

    def __str__(self) -> str:
        """
        String representation for the Search Space
        """
        return "Hyperparameter: %s | Range: %s | Default: %s | log: %s" % (
            self.hyperparameter, self.value_range, self.default_value, self.log)


def replace_prefix_in_config_dict(config: Dict[str, Any], prefix: str, replace: str = "") -> Dict[str, Any]:
    """
    Replace the prefix in all keys with the specified replacement string (the empty string by
    default to remove the prefix from the key). The functions makes sure that the prefix is a proper config
    prefix by checking if it ends with ":", if not it appends ":" to the prefix.

    :param config: config dictionary where the prefixed of the keys should be replaced
    :param prefix: prefix to be replaced in each key
    :param replace: the string to replace the prefix with
    :return: updated config dictionary
    """
    # make sure that prefix ends with the config separator ":"
    if not prefix.endswith(":"):
        prefix = prefix + ":"
    # only replace first occurrence of the prefix
    return {k.replace(prefix, replace, 1): v
            for k, v in config.items() if
            k.startswith(prefix)}


def custom_collate_fn(batch: List) -> List[Optional[torch.tensor]]:
    """
    In the case of not providing a y tensor, in a
    dataset of form {X, y}, y would be None.

    This custom collate function allows to yield
    None data for functions that require only features,
    like predict.

    Args:
        batch (List): a batch from a dataset

    Returns:
        List[Optional[torch.Tensor]]
    """

    items = list(zip(*batch))

    # The feature will always be available
    items[0] = default_collate(items[0])
    if None in items[1]:
        items[1] = list(items[1])
    else:
        items[1] = default_collate(items[1])
    return items


def replace_string_bool_to_bool(dictionary: Dict[str, Any]) -> Dict[str, Any]:
    """
    Utility function to replace string-type bool to
    bool when a dict is read from json

    Args:
        dictionary (Dict[str, Any])
    Returns:
        Dict[str, Any]
    """
    for key, item in dictionary.items():
        if isinstance(item, str):
            if item.lower() == "true":
                dictionary[key] = True
            elif item.lower() == "false":
                dictionary[key] = False
    return dictionary


def hash_array_or_matrix(X: Union[np.ndarray, pd.DataFrame]) -> str:
    """
    Creates a hash for a given array.
    Used for dataset name in case none is specified
    Args:
        X: (Union[np.ndarray, pd.DataFrame])
            data

    Returns:
        (str): hash of the data as string
    """
    m = hashlib.md5()

    if hasattr(X, "iloc"):
        X = X.to_numpy()

    if scipy.sparse.issparse(X):
        m.update(X.indices)
        m.update(X.indptr)
        m.update(X.data)
        m.update(str(X.shape).encode('utf8'))
    else:
        if X.flags['C_CONTIGUOUS']:
            m.update(X.data)
            m.update(str(X.shape).encode('utf8'))
        else:
            X_tmp = np.ascontiguousarray(X.T)
            m.update(X_tmp.data)
            m.update(str(X_tmp.shape).encode('utf8'))

    hash = m.hexdigest()
    return hash


def get_device_from_fit_dictionary(X: Dict[str, Any]) -> torch.device:
    """
    Get a torch device object by checking if the fit dictionary specifies a device. If not, or if no GPU is available
    return a CPU device.

    Args:
        X (Dict[str, Any]): A fit dictionary to control how the pipeline is fitted

    Returns:
        torch.device: Device to be used for training/inference
    """
    if not torch.cuda.is_available():
        return torch.device("cpu")

    return torch.device(X.get("device", "cpu"))


def subsampler(data: Union[np.ndarray, pd.DataFrame, scipy.sparse.csr_matrix],
               x: Union[np.ndarray, List[int]]
               ) -> Union[np.ndarray, pd.DataFrame, scipy.sparse.csr_matrix]:
    return data[x] if isinstance(data, (np.ndarray, scipy.sparse.csr_matrix)) else data.iloc[x]


def get_hyperparameter(hyperparameter: HyperparameterSearchSpace,
                       hyperparameter_type: Type[Hyperparameter]) -> Hyperparameter:
    """
    Given a hyperparameter search space, return a ConfigSpace Hyperparameter
    Args:
        hyperparameter (HyperparameterSearchSpace):
            the search space for the hyperparameter
        hyperparameter_type (Hyperparameter):
            the type of the hyperparameter

    Returns:
        Hyperparameter
    """
    if len(hyperparameter.value_range) == 0:
        raise ValueError(hyperparameter.hyperparameter + ': The range has to contain at least one element')
    if len(hyperparameter.value_range) == 1 and hyperparameter_type != CategoricalHyperparameter:
        return Constant(hyperparameter.hyperparameter, hyperparameter.value_range[0])
    if len(hyperparameter.value_range) == 2 and hyperparameter.value_range[0] == hyperparameter.value_range[1]:
        return Constant(hyperparameter.hyperparameter, hyperparameter.value_range[0])
    if hyperparameter_type == CategoricalHyperparameter:
        return CategoricalHyperparameter(hyperparameter.hyperparameter,
                                         choices=hyperparameter.value_range,
                                         default_value=hyperparameter.default_value)
    if hyperparameter_type == UniformFloatHyperparameter:
        assert len(hyperparameter.value_range) == 2, \
            "Float HP range update for %s is specified by the two upper " \
            "and lower values. %s given." % (hyperparameter.hyperparameter, len(hyperparameter.value_range))
        return UniformFloatHyperparameter(hyperparameter.hyperparameter,
                                          lower=hyperparameter.value_range[0],
                                          upper=hyperparameter.value_range[1],
                                          log=hyperparameter.log,
                                          default_value=hyperparameter.default_value)
    if hyperparameter_type == UniformIntegerHyperparameter:
        assert len(hyperparameter.value_range) == 2, \
            "Int HP range update for %s is specified by the two upper " \
            "and lower values. %s given." % (hyperparameter.hyperparameter, len(hyperparameter.value_range))
        return UniformIntegerHyperparameter(hyperparameter.hyperparameter,
                                            lower=hyperparameter.value_range[0],
                                            upper=hyperparameter.value_range[1],
                                            log=hyperparameter.log,
                                            default_value=hyperparameter.default_value)
    raise ValueError('Unknown type: %s for hp %s' % (hyperparameter_type, hyperparameter.hyperparameter))


def add_hyperparameter(cs: ConfigurationSpace,
                       hyperparameter: HyperparameterSearchSpace,
                       hyperparameter_type: Type[Hyperparameter]) -> None:
    """
    Adds the given hyperparameter to the given configuration space

    Args:
        cs (ConfigurationSpace):
            Configuration space where the hyperparameter must be added
        hyperparameter (HyperparameterSearchSpace):
            search space of the hyperparameter
        hyperparameter_type (Hyperparameter):
            type of the hyperparameter

    Returns:
        None
    """
    return cs.add_hyperparameter(get_hyperparameter(hyperparameter, hyperparameter_type))
