<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Developer Documentation &#8212; AutoPyTorch 0.0.3 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/bootstrap-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="_static/gallery.css" />
    <link rel="stylesheet" type="text/css" href="_static/gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="_static/gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="_static/gallery-rendered-html.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="_static/bootstrap-3.3.7/js/bootstrap.min.js "></script>
<script type="text/javascript" src="_static/bootstrap-sphinx.js "></script>

  </head><body>
  
  <a href="https://github.com/automl/Auto-PyTorch"
     class="visible-desktop hidden-xs"><img
    id="gh-banner"
    style="position: absolute; top: 50px; right: 0; border: 0;"
    src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png"
    alt="Fork me on GitHub"></a>
  <script>
    // Adjust banner height.
    $(function () {
      var navHeight = $(".navbar .container").css("height");
      $("#gh-banner").css("top", navHeight);
    });
  </script>


  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html">
          Auto-PyTorch</a>
        <span class="navbar-text navbar-version pull-left"><b>0.0.3</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="index.html">Start</a></li>
                <li><a href="releases.html">Releases</a></li>
                <li><a href="installation.html">Installation</a></li>
                <li><a href="manual.html">Manual</a></li>
                <li><a href="examples/index.html">Examples</a></li>
                <li><a href="api.html">API</a></li>
                <li><a href="#">Dev</a></li>
                <li><a href="extending.html">Extending</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"></ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
      <div class="col-md-3">
        <div id="sidebar" class="bs-sidenav" role="complementary"><ul>
<li><a class="reference internal" href="#">Developer Documentation</a><ul>
<li><a class="reference internal" href="#building-individual-models">Building Individual Models</a></li>
<li><a class="reference internal" href="#building-the-ensemble-model">Building the ensemble model</a></li>
<li><a class="reference internal" href="#the-automl-part">The AutoML Part</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    <div class="body col-md-9 content" role="main">
      
  <section id="developer-documentation">
<span id="dev"></span><h1>Developer Documentation<a class="headerlink" href="#developer-documentation" title="Permalink to this headline">¶</a></h1>
<p>This documentation summarizes how the AutoPyTorch code works, and is meant as a guide for the developers to help contribute to it.</p>
<p>AutoPyTorch relies on the <a class="reference external" href="https://automl.github.io/SMAC3/master/">SMAC</a> library to build individual models,
which are later ensembled together using ensemble selection by <a class="reference external" href="https://dl.acm.org/doi/pdf/10.1145/1015330.1015432">Caruana et al. (2004)</a>.
Therefore, there are two main parts of the code: <cite>AutoMLSMBO</cite>,  which acts as an interface to SMAC, and
<cite>EnsembleBuilder</cite> which opportunistically builds an ensemble of the individual algorithms found by SMAC, at fixed intervals.
The following sections provides details regarding this two main blocks of code.</p>
<section id="building-individual-models">
<h2>Building Individual Models<a class="headerlink" href="#building-individual-models" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>AutoPyTorch relies on Scikit-Learn <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html">Pipeline</a> to build an individual algorithm.</dt><dd><p>In other words, each of the individual models fitted by SMAC is (and comply) with Scikit-Learn pipeline and framework.</p>
</dd>
<dt>A pipeline can consist of various preprocessing steps including imputation, encoding, scaling,       feature preprocessing, and algorithm setup and training.  Regarding the training, Auto-PyTorch can fit 3 types of pipelines: a dummy pipeline, traditional classification pipelines, and PyTorch neural networks. The dummy pipeline builds on top of sklearn.dummy to construct an estimator that predicts using simple rules. This prediction is used as a baseline to define the worst-performing model that can be fit. Additionally, Auto-PyTorch fits traditional machine learning models (including           LightGBM, CatBoost, RandomForest, ExtraTrees, K-Nearest-Neighbors, and Support Vector Machines)       which are critical for small-sized datasets. The final type of machine learning pipeline corresponds to Neural Architecture Search of backbones (feature extraction) and network heads (for the final prediction). A pipeline might also contain additional training components like learning</dt><dd><p>rate scheduler, optimizers, and data loaders required to perform the neural architecture search.</p>
</dd>
</dl>
<p>In the case of tabular classification/regression, the training data is preprocessed using scikit-learn.compose.ColumnTransformer on a per-column basis. The data preprocessing is dynamically created depending on the dataset properties. For example, on a dataset that only contains float-type features, no one-hot encoding is needed. Additionally, we wrap the ColumnTransformer via              TabularColumnTransformer class to support torchvision transformation and handle column-reordering (Categorical columns are shifted to the left if one uses a ColumnTransformer).</p>
<p>When a pipeline is fitted, we use pickle to save it to disk as stated <a class="reference external" href="https://scikit-learn.org/stable/modules/model_persistence.html">here</a>. SMAC runs an optimization loop that proposes new configurations based on Bayesian optimization, which comply with the package <a class="reference external" href="https://automl.github.io/ConfigSpace/master/">ConfigSpace</a>. These configurations are then translated to AutoPyTorch pipelines, fitted, and finally saved to disc using the function evaluator <cite>ExecuteTaFuncWithQueue</cite>.  The latter is basically a worker that reads a dataset from disc, fits a pipeline, and collects the performance result which is communicated back to the main process via a Queue. This worker manages resources using <a class="reference external" href="https://github.com/automl/pynisher">Pynisher</a>, and it usually does so by creating a new process with a restricted memory (<cite>memory_limit</cite> API argument) and time constraints   (<cite>func_eval_time_limit_secs</cite> API argument).</p>
<p>The Scikit-learn pipeline inherits from the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">BaseEstimator</a>, which implies that we have to honor the <a class="reference external" href="https://scikit-learn.org/stable/developers/develop.html">Scikit-Learn development Guidelines</a>. Particularly, the arguments to the class constructor of any estimator must be defined as attributes of the class (see <cite>get_params and set_params</cite> from the above documentation).</p>
<p>To speed up the search, AutoPyTorch and SMAC use <a class="reference external" href="https://distributed.dask.org/en/latest/">Dask.distributed</a> multiprocessing scheme. We only submits jobs to Dask.distributed.Client up to the number of
workers, and wait for a worker to be available before continuing searching for more pipelines.</p>
<p>At the end of SMAC, the results will be available in the <cite>temporary_directory</cite> provided to the API run, in particular inside of the <cite>&lt;temporary_directory&gt;/smac3-output/run_&lt;SEED&gt;/</cite> directory. One can debug
the performance of the individual models using the file <cite>runhistory.json</cite> located in this area. Every individual model will be stored in <cite>&lt;temporary_directory&gt;/.autoPyTorch/runs</cite>.
In this <cite>runs</cite> directory we store the fitted model (during cross-validation we store a single Voting Classifier/Regressor, which is the soft voting outcome of k-Fold cross-validation), the Out-Of-Fold
predictions that are used to build an ensemble, and also the test predictions of this model in question.</p>
</section>
<section id="building-the-ensemble-model">
<h2>Building the ensemble model<a class="headerlink" href="#building-the-ensemble-model" title="Permalink to this headline">¶</a></h2>
<p>At every smac iteration, we submit a callback to create an ensemble in the case new models are written to disk. If no new models are available, no ensemble selection
is triggered. We use the OutOfFold predictions to build an ensemble via <cite>EnsembleSelection</cite>. This process is also submitted to Dask. Every new ensemble that is fitted is also
written to disk, where this object is mainly a container that specifies the weights one should use, to join individual model predictions.</p>
</section>
<section id="the-automl-part">
<h2>The AutoML Part<a class="headerlink" href="#the-automl-part" title="Permalink to this headline">¶</a></h2>
<p>The ensemble builder and the individual model constructions are both regulated by the <cite>BaseTask</cite>. This entity fundamentally calls the aforementioned task, and wait until
the time resource is exhausted.</p>
<p>We also rely on the <a class="reference external" href="https://automl.github.io/ConfigSpace/master/index.html">ConfigSpace</a> package to build a configuration space and sample configurations from it. A configuration in this context, determines the content of a pipeline (for example, that the final estimator will be a MLP, or that it will have PCA as preprocessing).The set of valid configurations is determined by the configuration space. The configuration space is build using the dataset characteristics, like type
of features (categorical, numerical) or the target type (classification, regression).</p>
</section>
</section>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="_sources/dev.rst.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2014-2019, Machine Learning Professorship Freiburg.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.0.1.<br/>
    </p>
  </div>
</footer>
  </body>
</html>