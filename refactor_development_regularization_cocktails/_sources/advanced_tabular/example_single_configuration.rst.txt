
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "advanced_tabular/example_single_configuration.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_advanced_tabular_example_single_configuration.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_advanced_tabular_example_single_configuration.py:


==========================
Fit a single configuration
==========================
*Auto-PyTorch* searches for the best combination of machine learning algorithms
and their hyper-parameter configuration for a given task.

This example shows how one can fit one of these pipelines, both, with a user defined
configuration, and a randomly sampled one form the configuration space.
The pipelines that Auto-PyTorch fits are compatible with Scikit-Learn API. You can
get further documentation about Scikit-Learn models here: <https://scikit-learn.org/stable/getting_started.html`>_

.. GENERATED FROM PYTHON SOURCE LINES 14-90




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/runner/work/Auto-PyTorch/Auto-PyTorch/examples/tabular/40_advanced/example_single_configuration.py:61: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      configuration = estimator.get_search_space(dataset).get_default_configuration()
    /home/runner/work/Auto-PyTorch/Auto-PyTorch/examples/tabular/40_advanced/example_single_configuration.py:61: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      configuration = estimator.get_search_space(dataset).get_default_configuration()
    {'imputer': SimpleImputer(random_state=RandomState(MT19937) at 0x7FAB7BFD7D40), 'encoder': <autoPyTorch.pipeline.components.preprocessing.tabular_preprocessing.encoding.base_encoder_choice.EncoderChoice object at 0x7fab79c15be0>, 'scaler': <autoPyTorch.pipeline.components.preprocessing.tabular_preprocessing.scaling.base_scaler_choice.ScalerChoice object at 0x7fab79e66700>, 'feature_preprocessor': <autoPyTorch.pipeline.components.preprocessing.tabular_preprocessing.feature_preprocessing.base_feature_preprocessor_choice.FeatureProprocessorChoice object at 0x7fab88507e20>, 'tabular_transformer': TabularColumnTransformer(random_state=RandomState(MT19937) at 0x7FAB7BFD7D40), 'preprocessing': EarlyPreprocessing(random_state=RandomState(MT19937) at 0x7FAB7BFD7D40), 'network_embedding': <autoPyTorch.pipeline.components.setup.network_embedding.base_network_embedding_choice.NetworkEmbeddingChoice object at 0x7fab79e8be20>, 'network_backbone': <autoPyTorch.pipeline.components.setup.network_backbone.base_network_backbone_choice.NetworkBackboneChoice object at 0x7fab8808f7f0>, 'network_head': <autoPyTorch.pipeline.components.setup.network_head.base_network_head_choice.NetworkHeadChoice object at 0x7fab7bf46550>, 'network': NetworkComponent(network=Sequential(
      (0): _NoEmbedding()
      (1): Sequential(
        (0): Linear(in_features=4, out_features=200, bias=True)
        (1): ReLU()
        (2): Linear(in_features=200, out_features=200, bias=True)
        (3): ReLU()
        (4): Linear(in_features=200, out_features=200, bias=True)
        (5): ReLU()
        (6): Linear(in_features=200, out_features=200, bias=True)
        (7): ReLU()
        (8): Linear(in_features=200, out_features=200, bias=True)
      )
      (2): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=200, out_features=128, bias=True)
        (2): ReLU()
        (3): Linear(in_features=128, out_features=3, bias=True)
      )
    ),
                     network_snapshots=[],
                     random_state=RandomState(MT19937) at 0x7FAB7BFD7D40), 'network_init': <autoPyTorch.pipeline.components.setup.network_initializer.base_network_init_choice.NetworkInitializerChoice object at 0x7fab7bf46f70>, 'optimizer': <autoPyTorch.pipeline.components.setup.optimizer.base_optimizer_choice.OptimizerChoice object at 0x7fab79c762b0>, 'lr_scheduler': <autoPyTorch.pipeline.components.setup.lr_scheduler.base_scheduler_choice.SchedulerChoice object at 0x7fab7bf21b20>, 'data_loader': FeatureDataLoader(random_state=RandomState(MT19937) at 0x7FAB7BFD7D40), 'trainer': <autoPyTorch.pipeline.components.training.trainer.base_trainer_choice.TrainerChoice object at 0x7fab7b95b610>}
    RunInfo(config=Configuration:
      data_loader:batch_size, Value: 64
      encoder:__choice__, Value: 'NoEncoder'
      feature_preprocessor:__choice__, Value: 'NoFeaturePreprocessor'
      imputer:numerical_strategy, Value: 'mean'
      lr_scheduler:ReduceLROnPlateau:factor, Value: 0.1
      lr_scheduler:ReduceLROnPlateau:mode, Value: 'min'
      lr_scheduler:ReduceLROnPlateau:patience, Value: 10
      lr_scheduler:__choice__, Value: 'ReduceLROnPlateau'
      network_backbone:ShapedMLPBackbone:activation, Value: 'relu'
      network_backbone:ShapedMLPBackbone:max_units, Value: 200
      network_backbone:ShapedMLPBackbone:mlp_shape, Value: 'funnel'
      network_backbone:ShapedMLPBackbone:num_groups, Value: 5
      network_backbone:ShapedMLPBackbone:output_dim, Value: 200
      network_backbone:ShapedMLPBackbone:use_dropout, Value: False
      network_backbone:__choice__, Value: 'ShapedMLPBackbone'
      network_embedding:__choice__, Value: 'NoEmbedding'
      network_head:__choice__, Value: 'fully_connected'
      network_head:fully_connected:activation, Value: 'relu'
      network_head:fully_connected:num_layers, Value: 2
      network_head:fully_connected:units_layer_1, Value: 128
      network_init:XavierInit:bias_strategy, Value: 'Normal'
      network_init:__choice__, Value: 'XavierInit'
      optimizer:AdamOptimizer:beta1, Value: 0.9
      optimizer:AdamOptimizer:beta2, Value: 0.9
      optimizer:AdamOptimizer:lr, Value: 0.01
      optimizer:AdamOptimizer:use_weight_decay, Value: True
      optimizer:AdamOptimizer:weight_decay, Value: 0.0001
      optimizer:__choice__, Value: 'AdamOptimizer'
      scaler:__choice__, Value: 'StandardScaler'
      trainer:StandardTrainer:Lookahead:la_alpha, Value: 0.6
      trainer:StandardTrainer:Lookahead:la_steps, Value: 6
      trainer:StandardTrainer:se_lastk, Constant: 3
      trainer:StandardTrainer:use_lookahead_optimizer, Value: True
      trainer:StandardTrainer:use_snapshot_ensemble, Value: True
      trainer:StandardTrainer:use_stochastic_weight_averaging, Value: True
      trainer:StandardTrainer:weighted_loss, Value: True
      trainer:__choice__, Value: 'StandardTrainer'
    , instance=None, instance_specific=None, seed=1, cutoff=94, capped=False, budget=50, source_id=0)
    RunValue(cost=0.0, time=7.275488615036011, status=<StatusType.SUCCESS: 1>, starttime=1621005919.9358788, endtime=1621005928.576877, additional_info={'accuracy': 0.0, 'duration': 4.888242959976196, 'num_run': 2, 'train_loss': {'accuracy': 0.0}, 'test_loss': 0.125, 'configuration_origin': None})
    Passed Configuration: Configuration:
      data_loader:batch_size, Value: 64
      encoder:__choice__, Value: 'NoEncoder'
      feature_preprocessor:__choice__, Value: 'NoFeaturePreprocessor'
      imputer:numerical_strategy, Value: 'mean'
      lr_scheduler:ReduceLROnPlateau:factor, Value: 0.1
      lr_scheduler:ReduceLROnPlateau:mode, Value: 'min'
      lr_scheduler:ReduceLROnPlateau:patience, Value: 10
      lr_scheduler:__choice__, Value: 'ReduceLROnPlateau'
      network_backbone:ShapedMLPBackbone:activation, Value: 'relu'
      network_backbone:ShapedMLPBackbone:max_units, Value: 200
      network_backbone:ShapedMLPBackbone:mlp_shape, Value: 'funnel'
      network_backbone:ShapedMLPBackbone:num_groups, Value: 5
      network_backbone:ShapedMLPBackbone:output_dim, Value: 200
      network_backbone:ShapedMLPBackbone:use_dropout, Value: False
      network_backbone:__choice__, Value: 'ShapedMLPBackbone'
      network_embedding:__choice__, Value: 'NoEmbedding'
      network_head:__choice__, Value: 'fully_connected'
      network_head:fully_connected:activation, Value: 'relu'
      network_head:fully_connected:num_layers, Value: 2
      network_head:fully_connected:units_layer_1, Value: 128
      network_init:XavierInit:bias_strategy, Value: 'Normal'
      network_init:__choice__, Value: 'XavierInit'
      optimizer:AdamOptimizer:beta1, Value: 0.9
      optimizer:AdamOptimizer:beta2, Value: 0.9
      optimizer:AdamOptimizer:lr, Value: 0.01
      optimizer:AdamOptimizer:use_weight_decay, Value: True
      optimizer:AdamOptimizer:weight_decay, Value: 0.0001
      optimizer:__choice__, Value: 'AdamOptimizer'
      scaler:__choice__, Value: 'StandardScaler'
      trainer:StandardTrainer:Lookahead:la_alpha, Value: 0.6
      trainer:StandardTrainer:Lookahead:la_steps, Value: 6
      trainer:StandardTrainer:se_lastk, Constant: 3
      trainer:StandardTrainer:use_lookahead_optimizer, Value: True
      trainer:StandardTrainer:use_snapshot_ensemble, Value: True
      trainer:StandardTrainer:use_stochastic_weight_averaging, Value: True
      trainer:StandardTrainer:weighted_loss, Value: True
      trainer:__choice__, Value: 'StandardTrainer'

    Network: Sequential(
      (0): _NoEmbedding()
      (1): Sequential(
        (0): Linear(in_features=4, out_features=200, bias=True)
        (1): ReLU()
        (2): Linear(in_features=200, out_features=200, bias=True)
        (3): ReLU()
        (4): Linear(in_features=200, out_features=200, bias=True)
        (5): ReLU()
        (6): Linear(in_features=200, out_features=200, bias=True)
        (7): ReLU()
        (8): Linear(in_features=200, out_features=200, bias=True)
      )
      (2): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=200, out_features=128, bias=True)
        (2): ReLU()
        (3): Linear(in_features=128, out_features=3, bias=True)
      )
    )






|

.. code-block:: default

    import os
    import tempfile as tmp
    import warnings

    os.environ['JOBLIB_TEMP_FOLDER'] = tmp.gettempdir()
    os.environ['OMP_NUM_THREADS'] = '1'
    os.environ['OPENBLAS_NUM_THREADS'] = '1'
    os.environ['MKL_NUM_THREADS'] = '1'

    warnings.simplefilter(action='ignore', category=UserWarning)
    warnings.simplefilter(action='ignore', category=FutureWarning)

    import sklearn.datasets
    import sklearn.metrics

    from autoPyTorch.api.tabular_classification import TabularClassificationTask
    from autoPyTorch.datasets.resampling_strategy import HoldoutValTypes


    if __name__ == '__main__':
        ############################################################################
        # Data Loading
        # ============

        X, y = sklearn.datasets.fetch_openml('iris', return_X_y=True, as_frame=True)
        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
            X, y, test_size=0.8, random_state=3
        )

        ############################################################################
        # Define an estimator
        # ============================

        # Search for a good configuration
        estimator = TabularClassificationTask(
            resampling_strategy=HoldoutValTypes.holdout_validation,
            resampling_strategy_args={'val_share': 0.33}
        )

        ############################################################################
        # Get a random configuration of the pipeline for current dataset
        # ===============================================================

        dataset = estimator.get_dataset(X_train=X_train,
                                        y_train=y_train,
                                        X_test=X_test,
                                        y_test=y_test)
        configuration = estimator.get_search_space(dataset).get_default_configuration()

        ###########################################################################
        # Fit the configuration
        # ==================================

        pipeline, run_info, run_value, dataset = estimator.fit_pipeline(
            X_train=X_train,
            y_train=y_train,
            dataset_name='kr-vs-kp',
            run_time_limit_secs=100,
            X_test=X_test,
            y_test=y_test,
            disable_file_output=False,
            configuration=configuration,
        )

        # This object complies with Scikit-Learn Pipeline API.
        # https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html
        print(pipeline.named_steps)

        # The fit_pipeline command also returns a named tuple with the pipeline constraints
        print(run_info)

        # The fit_pipeline command also returns a named tuple with train/test performance
        print(run_value)

        print("Passed Configuration:", pipeline.config)
        print("Network:", pipeline.named_steps['network'].network)


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  20.915 seconds)


.. _sphx_glr_download_advanced_tabular_example_single_configuration.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example


  .. container:: binder-badge

    .. image:: images/binder_badge_logo.svg
      :target: https://mybinder.org/v2/gh/automl/Auto-PyTorch/refactor_development?urlpath=lab/tree/notebooks/advanced_tabular/example_single_configuration.ipynb
      :alt: Launch binder
      :width: 150 px


  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: example_single_configuration.py <example_single_configuration.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: example_single_configuration.ipynb <example_single_configuration.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
